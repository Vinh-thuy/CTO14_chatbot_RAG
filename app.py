import os
import torch
from flask import Flask, request, jsonify
from flasgger import Swagger
from flasgger.utils import swag_from
from llm_rag_demo import load_llm, setup_rag_system, rag_query, generate_response
from sentence_transformers import SentenceTransformer
import logging
import chromadb

# Configuration de l'application Flask
app = Flask(__name__)
swagger_config = {
    "headers": [],
    "specs": [
        {
            "endpoint": 'apispec_1',
            "route": '/apispec_1.json',
            "rule_filter": lambda rule: True,  # all in
            "model_filter": lambda tag: True,  # all in
        }
    ],
    "static_url_path": "/flasgger_static",
    "swagger_ui": True,
    "specs_route": "/apidocs/"
}
Swagger(app, config=swagger_config)

# Chargement initial des mod√®les
try:
    llm_model, llm_tokenizer = load_llm()
except Exception as e:
    print(f"Erreur lors du chargement initial des mod√®les : {e}")
    llm_model, llm_tokenizer = None, None

logger = logging.getLogger(__name__)

# Initialisation globale de la base vectorielle
rag_collection = None
embedder = None

# Wrapper pour la fonction d'embedding compatible ChromaDB
class EmbeddingFunctionWrapper:
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
    
    def __call__(self, input):
        """
        M√©thode compatible avec l'interface ChromaDB
        
        Args:
            input (List[str]): Liste de textes √† encoder
        
        Returns:
            List[List[float]]: Liste d'embeddings
        """
        try:
            # Encoder tous les textes d'un coup
            embeddings = self.embedding_model.encode(input)
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'embedding : {e}")
            raise

def initialize_rag_system():
    """
    Initialise le syst√®me RAG une seule fois au d√©marrage
    """
    global rag_collection, embedder
    
    try:
        # Charger l'embedder
        embedder = SentenceTransformer('all-MiniLM-L6-v2')
        logger.info("‚úÖ Mod√®le d'embedding initialis√©")
        
        # Cr√©er un wrapper compatible ChromaDB
        embedding_function = EmbeddingFunctionWrapper(embedder)
        
        # Cr√©er ou charger la collection vectorielle
        rag_collection = chromadb.PersistentClient(path="./vectorstore").get_or_create_collection(
            name="document_collection", 
            embedding_function=embedding_function
        )
        logger.info(f"‚úÖ Collection vectorielle initialis√©e avec {rag_collection.count()} segments")
        
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur d'initialisation RAG : {e}")
        return False

# Initialiser le syst√®me RAG au d√©marrage
rag_initialization_success = initialize_rag_system()

def get_model_size(model):
    """
    Calcule la taille approximative d'un mod√®le
    
    Args:
        model: Mod√®le PyTorch ou Transformers
    
    Returns:
        dict: Informations sur la taille du mod√®le
    """
    try:
        # Calcul de la taille totale des param√®tres
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # Estimation de la taille m√©moire
        param_size = total_params * 4 / (1024 ** 2)  # en Mo (float32)
        
        return {
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "estimated_size_mb": round(param_size, 2),
            "device": str(next(model.parameters()).device)
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du calcul de la taille du mod√®le : {e}")
        return None

@app.route('/generate', methods=['POST'])
@swag_from({
    'tags': ['LLM Generation'],
    'description': 'G√©n√©rer une r√©ponse textuelle avec un mod√®le de langage',
    'parameters': [
        {
            'name': 'body',
            'in': 'body',
            'required': True,
            'schema': {
                'type': 'object',
                'properties': {
                    'prompt': {
                        'type': 'string',
                        'description': 'Texte de d√©part pour la g√©n√©ration',
                        'example': 'Explique les bases de l\'intelligence artificielle'
                    }
                },
                'required': ['prompt']
            }
        }
    ],
    'responses': {
        '200': {
            'description': 'R√©ponse g√©n√©r√©e avec succ√®s',
            'schema': {
                'type': 'object',
                'properties': {
                    'response': {
                        'type': 'string',
                        'description': 'Texte g√©n√©r√© par le mod√®le'
                    },
                    'prompt': {
                        'type': 'string',
                        'description': 'Prompt original'
                    }
                },
                'example': {
                    'response': 'L\'intelligence artificielle est un domaine...',
                    'prompt': 'Explique les bases de l\'intelligence artificielle'
                }
            }
        },
        '400': {
            'description': 'Requ√™te invalide',
            'schema': {
                'type': 'object',
                'properties': {
                    'error': {
                        'type': 'string',
                        'description': 'Message d\'erreur'
                    }
                },
                'example': {
                    'error': 'Param√®tre \'prompt\' requis'
                }
            }
        },
        '500': {
            'description': 'Erreur serveur',
            'schema': {
                'type': 'object',
                'properties': {
                    'error': {
                        'type': 'string',
                        'description': 'Message d\'erreur d√©taill√©'
                    }
                },
                'example': {
                    'error': 'Mod√®le LLM non configur√©'
                }
            }
        }
    }
})
def generate_text():
    """
    Endpoint pour g√©n√©rer du texte avec le mod√®le LLM
    """
    if not llm_model or not llm_tokenizer:
        return jsonify({"error": "Mod√®le LLM non initialis√©"}), 500
    
    data = request.get_json()
    prompt = data.get('prompt', '')
    
    if not prompt:
        return jsonify({"error": "Prompt requis"}), 400
    
    try:
        response = generate_response(llm_model, llm_tokenizer, prompt)
        return jsonify({"response": response, "prompt": prompt})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/rag_query', methods=['POST'])
@swag_from({
    'tags': ['RAG Query'],
    'description': 'Effectuer une requ√™te RAG avec recherche s√©mantique',
    'parameters': [
        {
            'name': 'body',
            'in': 'body',
            'required': True,
            'schema': {
                'type': 'object',
                'properties': {
                    'query': {
                        'type': 'string',
                        'description': 'Question ou requ√™te pour la recherche s√©mantique',
                        'example': 'Qu\'est-ce que le RAG ?'
                    }
                },
                'required': ['query']
            }
        }
    ],
    'responses': {
        '200': {
            'description': 'R√©ponse RAG g√©n√©r√©e avec succ√®s',
            'schema': {
                'type': 'object',
                'properties': {
                    'rag_response': {
                        'type': 'string',
                        'description': 'R√©ponse g√©n√©r√©e par le syst√®me RAG'
                    },
                    'query': {
                        'type': 'string',
                        'description': 'Requ√™te originale'
                    }
                },
                'example': {
                    'rag_response': 'Le RAG (Retrieval-Augmented Generation) est une technique...',
                    'query': 'Qu\'est-ce que le RAG ?'
                }
            }
        },
        '400': {
            'description': 'Requ√™te invalide',
            'schema': {
                'type': 'object',
                'properties': {
                    'error': {
                        'type': 'string',
                        'description': 'Message d\'erreur'
                    }
                },
                'example': {
                    'error': 'Param√®tre \'query\' requis'
                }
            }
        },
        '500': {
            'description': 'Erreur serveur',
            'schema': {
                'type': 'object',
                'properties': {
                    'error': {
                        'type': 'string',
                        'description': 'Message d\'erreur d√©taill√©'
                    }
                },
                'example': {
                    'error': 'Syst√®me RAG non configur√©'
                }
            }
        }
    }
})
def semantic_search():
    """
    Endpoint pour effectuer une requ√™te RAG
    """
    # V√©rification de l'initialisation du syst√®me RAG
    if not rag_initialization_success:
        logger.error("üö® Initialisation RAG a √©chou√©")
        return jsonify({"error": "Syst√®me RAG non configur√©"}), 500
    
    # V√©rification des d√©pendances
    if not rag_collection:
        logger.error("üö® Collection RAG non initialis√©e")
        return jsonify({"error": "Syst√®me RAG non configur√©"}), 500
    
    if not embedder:
        logger.error("üö® Mod√®le d'embedding non initialis√©")
        return jsonify({"error": "Embedding non configur√©"}), 500
    
    if not llm_model or not llm_tokenizer:
        logger.error("üö® Mod√®le LLM non initialis√©")
        return jsonify({"error": "Mod√®le LLM non configur√©"}), 500
    
    # R√©cup√©ration des donn√©es
    data = request.get_json()
    
    # Validation de la requ√™te
    if not data:
        logger.warning("‚ö†Ô∏è Requ√™te vide re√ßue")
        return jsonify({"error": "Corps de requ√™te vide"}), 400
    
    query = data.get('query', '').strip()
    
    if not query:
        logger.warning("‚ö†Ô∏è Param√®tre 'query' manquant ou vide")
        return jsonify({"error": "Param√®tre 'query' requis"}), 400
    
    # Logging de la requ√™te
    logger.info(f"üîç Requ√™te RAG re√ßue : {query}")
    
    try:
        # Ex√©cution de la requ√™te RAG
        rag_response = rag_query(
            rag_collection, 
            embedder, 
            llm_model, 
            llm_tokenizer, 
            query
        )
        
        # Logging du r√©sultat
        logger.info(f"‚úÖ R√©ponse RAG g√©n√©r√©e avec succ√®s")
        
        return jsonify({
            "rag_response": rag_response,
            "query": query
        })
    
    except Exception as e:
        # Gestion des erreurs d√©taill√©e
        logger.error(f"‚ùå Erreur lors de la requ√™te RAG : {str(e)}")
        return jsonify({
            "error": "Erreur lors du traitement de la requ√™te RAG",
            "details": str(e)
        }), 500

@app.route('/model_info', methods=['GET'])
def model_info():
    """
    Endpoint pour obtenir les informations du mod√®le LLM
    """
    if not llm_model:
        return jsonify({"error": "Mod√®le non initialis√©"}), 500
    
    model_details = get_model_size(llm_model)
    
    if not model_details:
        return jsonify({"error": "Impossible de r√©cup√©rer les informations du mod√®le"}), 500
    
    return jsonify({
        "model_name": llm_model.config.model_type,
        "model_size": model_details
    })

@app.route('/', methods=['GET'])
def home():
    """
    Page d'accueil avec des informations sur l'API
    """
    return """
    <h1>ü§ñ LLM RAG API</h1>
    <p>Bienvenue sur l'API de d√©monstration RAG !</p>
    <ul>
        <li><a href="/apidocs/">üìò Documentation Swagger</a></li>
        <li><strong>/generate</strong>: G√©n√©rer du texte avec un LLM</li>
        <li><strong>/rag_query</strong>: Effectuer une recherche s√©mantique</li>
        <li><strong>/model_info</strong>: Obtenir les informations du mod√®le</li>
    </ul>
    """

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5001)
