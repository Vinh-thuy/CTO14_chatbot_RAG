# DÃ©monstration de Retrieval-Augmented Generation (RAG) avec LLM

## ğŸš€ Introduction

Ce projet est une dÃ©monstration pratique de Retrieval-Augmented Generation (RAG), une technique innovante qui amÃ©liore les capacitÃ©s des modÃ¨les de langage en leur fournissant un contexte supplÃ©mentaire lors de la gÃ©nÃ©ration de rÃ©ponses.

## ğŸ“˜ Concepts ClÃ©s

### Qu'est-ce que le RAG ?
Le RAG combine deux technologies puissantes :
1. **Recherche sÃ©mantique** : Trouver des informations pertinentes dans un document
2. **GÃ©nÃ©ration de texte** : Produire une rÃ©ponse basÃ©e sur ces informations

### Composants Principaux
- **ModÃ¨le de Langage** : `distilgpt2` (version allÃ©gÃ©e de GPT-2)
- **Embeddings** : Transformations vectorielles des documents
- **Base de DonnÃ©es Vectorielle** : ChromaDB pour le stockage et la recherche sÃ©mantique

## ğŸ›  FonctionnalitÃ©s Principales

### 1. Chargement du ModÃ¨le LLM
- TÃ©lÃ©chargement du modÃ¨le `distilgpt2`
- Configuration optimisÃ©e pour les performances
- Gestion intelligente du cache

### 2. Extraction de Texte PDF
- Lecture automatique de documents PDF
- Extraction du contenu textuel

### 3. Indexation SÃ©mantique
- Transformation du texte en vecteurs
- CrÃ©ation d'une base de donnÃ©es vectorielle
- Permet une recherche sÃ©mantique prÃ©cise

### 4. GÃ©nÃ©ration de RÃ©ponses
- Deux modes de gÃ©nÃ©ration :
  1. **Sans Contexte** : RÃ©ponse basique du modÃ¨le
  2. **Avec RAG** : RÃ©ponse enrichie par des informations contextuelles

## ğŸ” Workflow DÃ©taillÃ©

1. **PrÃ©paration**
   - Charger le modÃ¨le de langage
   - Initialiser l'embedder
   - Extraire le texte du PDF

2. **Indexation**
   - Diviser le texte en segments
   - CrÃ©er des embeddings
   - Stocker dans ChromaDB

3. **RequÃªte**
   - L'utilisateur pose une question
   - Recherche sÃ©mantique des passages pertinents
   - GÃ©nÃ©ration de la rÃ©ponse avec contexte

## ğŸ”¬ Architecture Technique DÃ©taillÃ©e

### 1. SystÃ¨me de Logging PersonnalisÃ© ğŸŒˆ

#### Objectif : AmÃ©lioration de la LisibilitÃ©
- **Codes Couleurs PersonnalisÃ©s** :
  - ğŸŸ£ Magenta : Chargement du modÃ¨le Hugging Face
  - ğŸ”µ Bleu : RequÃªtes LLM
  - ğŸŸ¢ Vert : Configuration du RAG
  - ğŸ”· Cyan : RequÃªtes RAG

#### Avantages
- Distinction visuelle des Ã©tapes d'exÃ©cution
- AmÃ©lioration du dÃ©bogage
- ExpÃ©rience utilisateur amÃ©liorÃ©e

### 2. Chargement du ModÃ¨le LLM ğŸ¤–

#### Ã‰tapes Principales
1. **PrÃ©paration du Cache**
   - CrÃ©ation d'un rÃ©pertoire de cache personnalisÃ©
   - Nettoyage du cache existant

2. **Chargement du Tokenizer**
   - Utilisation de `AutoTokenizer`
   - Configuration des tokens spÃ©ciaux
   - Logging dÃ©taillÃ© :
     ```
     ğŸ¤– TÃ©lÃ©chargement du tokenizer...
     âœ… Tokenizer tÃ©lÃ©chargÃ©
     ğŸ“Š Taille du vocabulaire
     ğŸ·ï¸ Tokens spÃ©ciaux
     ```

3. **Optimisations du ModÃ¨le**
   - `device_map='auto'` : SÃ©lection automatique du pÃ©riphÃ©rique
   - `torch_dtype=torch.float16` : RÃ©duction de prÃ©cision
   - `low_cpu_mem_usage=True` : Minimisation de l'utilisation CPU

### 3. GÃ©nÃ©ration de RÃ©ponse ğŸ”

#### StratÃ©gies Intelligentes
- SÃ©lection dynamique du pÃ©riphÃ©rique (CPU/GPU/MPS)
- Tokenization adaptative
- Gestion des erreurs de gÃ©nÃ©ration

### 4. Extraction de Texte PDF ğŸ“„

#### Workflow
1. Ouverture du fichier PDF
2. Lecture sÃ©quentielle des pages
3. ConcatÃ©nation du texte
4. Gestion robuste des erreurs

### 5. Configuration du SystÃ¨me RAG ğŸ—ï¸

#### Processus d'Indexation
1. **PrÃ©paration du Texte**
   - Division en segments intelligents
   - Utilisation de `RecursiveCharacterTextSplitter`

2. **CrÃ©ation d'Embeddings**
   - Transformation vectorielle
   - ModÃ¨le `SentenceTransformer`

3. **Indexation Vectorielle**
   - Collection ChromaDB
   - Stockage des reprÃ©sentations sÃ©mantiques

### 6. RequÃªte RAG Intelligente ğŸ•µï¸

#### Ã‰tapes de Recherche et GÃ©nÃ©ration
1. Embedding de la requÃªte utilisateur
2. Recherche sÃ©mantique dans ChromaDB
3. RÃ©cupÃ©ration des passages contextuels
4. GÃ©nÃ©ration de rÃ©ponse enrichie

### 7. Comparaison Analytique ğŸ”¬

#### Objectif : Ã‰valuation Comparative
- Comparaison des rÃ©ponses :
  1. Sans contexte RAG
  2. Avec contexte RAG
- Mesure de la valeur ajoutÃ©e du RAG

### Principes de Conception ğŸ¯

#### Approche Technique
- **ModularitÃ©** : ResponsabilitÃ©s uniques par fonction
- **Logging Exhaustif** : TraÃ§abilitÃ© complÃ¨te
- **AdaptabilitÃ©** : Configuration dynamique
- **Performance** : Optimisation des ressources

### Points Techniques AvancÃ©s ğŸš€

- AccÃ©lÃ©ration GPU avec `torch`
- Gestion dynamique des pÃ©riphÃ©riques
- Logging colorÃ© et informatif
- Embeddings sÃ©mantiques avancÃ©s
- Recherche contextuelle intelligente

### Limitations Actuelles ğŸš§

- ModÃ¨le lÃ©ger (`distilgpt2`)
- Performances pour tÃ¢ches complexes
- DÃ©pendance Ã  la qualitÃ© du document source

## ğŸ”® Perspectives d'AmÃ©lioration

1. IntÃ©gration de modÃ¨les plus performants
2. AmÃ©lioration de la recherche sÃ©mantique
3. DÃ©veloppement de mÃ©triques d'Ã©valuation
4. Support multi-documents
5. Interface utilisateur plus intuitive

## ğŸŒ ImplÃ©mentation Technique de l'API Flask

### Architecture de l'API

#### Endpoints Principaux
1. **`/generate`** : GÃ©nÃ©ration de texte
   - **MÃ©thode** : POST
   - **Objectif** : GÃ©nÃ©rer du texte avec le modÃ¨le LLM
   - **ParamÃ¨tres** :
     - `prompt` (requis) : Texte de dÃ©part pour la gÃ©nÃ©ration

2. **`/rag_query`** : Recherche SÃ©mantique RAG
   - **MÃ©thode** : POST
   - **Objectif** : Effectuer une recherche sÃ©mantique contextuelle
   - **ParamÃ¨tres** :
     - `query` (requis) : Question ou requÃªte pour la recherche

3. **`/model_info`** : Informations du ModÃ¨le
   - **MÃ©thode** : GET
   - **Objectif** : Fournir des informations dÃ©taillÃ©es sur le modÃ¨le de langage utilisÃ©
   - **ParamÃ¨tres** : Aucun

### Format des RequÃªtes ğŸ“‹

#### Structure JSON des Endpoints ğŸ“‹

#### 1. Endpoint `/generate`

##### JSON d'EntrÃ©e
```json
{
  "prompt": "Votre texte de dÃ©part pour la gÃ©nÃ©ration"
}
```

##### JSON de RÃ©ponse (SuccÃ¨s)
```json
{
  "response": "Texte gÃ©nÃ©rÃ© par le modÃ¨le LLM",
  "prompt": "Votre texte de dÃ©part"
}
```

##### JSON de RÃ©ponse (Erreur)
```json
{
  "error": "Message dÃ©crivant l'erreur"
}
```

#### 2. Endpoint `/rag_query`

##### JSON d'EntrÃ©e
```json
{
  "query": "Votre question ou requÃªte sÃ©mantique"
}
```

##### JSON de RÃ©ponse (SuccÃ¨s)
```json
{
  "rag_response": "RÃ©ponse gÃ©nÃ©rÃ©e par le systÃ¨me RAG",
  "query": "Votre question originale"
}
```

##### JSON de RÃ©ponse (Erreur)
```json
{
  "error": "Message dÃ©crivant l'erreur"
}
```

### Validation des RequÃªtes ğŸ›¡ï¸

#### RÃ¨gles GÃ©nÃ©rales
- Chaque endpoint attend un JSON valide
- Le champ requis doit Ãªtre non vide
- Le Content-Type doit Ãªtre `application/json`

#### Exemples de RequÃªtes Valides

```bash
# GÃ©nÃ©ration de texte
curl -X POST http://localhost:5001/generate \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Explique l\'intelligence artificielle"}'

# RequÃªte RAG
curl -X POST http://localhost:5001/rag_query \
     -H "Content-Type: application/json" \
     -d '{"query": "Qu\'est-ce que le RAG ?"}'
```

### Gestion des Erreurs ğŸš¨

#### Types d'Erreurs Courants
1. **400 Bad Request** : 
   - JSON invalide
   - Champ requis manquant
   - Champ vide
2. **500 Internal Server Error** :
   - ModÃ¨le non initialisÃ©
   - Erreur systÃ¨me
   - ProblÃ¨me de traitement

### DÃ©bogage des RequÃªtes ğŸ”

Si vous rencontrez des problÃ¨mes :
1. VÃ©rifiez le format JSON
2. Assurez-vous que le champ `query` ou `prompt` est prÃ©sent et non vide
3. Utilisez un validateur JSON en ligne
4. Consultez les logs du serveur pour des dÃ©tails spÃ©cifiques

### DÃ©bogage des RequÃªtes de GÃ©nÃ©ration ğŸ”

Si vous rencontrez des problÃ¨mes :
1. VÃ©rifiez le format JSON
2. Assurez-vous que le champ `prompt` est prÃ©sent et non vide
3. Utilisez un validateur JSON en ligne
4. Consultez les logs du serveur pour des dÃ©tails spÃ©cifiques

### Gestion des Erreurs et Logging ğŸš¨

#### Principes de Gestion des Erreurs
- Validation stricte des requÃªtes
- Messages d'erreur dÃ©taillÃ©s
- Logging complet des Ã©vÃ©nements

#### Types de Validation
- VÃ©rification de l'initialisation des modÃ¨les
- ContrÃ´le de la prÃ©sence et du format des paramÃ¨tres
- Gestion des exceptions durant le traitement

### Exemple de Workflow de RequÃªte

1. **Validation de la RequÃªte**
   - VÃ©rification de l'existence du modÃ¨le
   - Validation des paramÃ¨tres d'entrÃ©e
   - Logging de la requÃªte reÃ§ue

2. **Traitement**
   - GÃ©nÃ©ration de rÃ©ponse ou recherche sÃ©mantique
   - Capture des erreurs potentielles
   - Logging du rÃ©sultat

3. **RÃ©ponse**
   - Retour JSON structurÃ©
   - Codes de statut HTTP appropriÃ©s
   - Informations de dÃ©bogage si nÃ©cessaire

### SÃ©curitÃ© et Configuration ğŸ”’

#### Bonnes Pratiques
- Mode debug dÃ©sactivÃ© en production
- Potential configuration CORS
- PrÃ©paration pour l'ajout d'authentification

#### DÃ©ploiement
- Serveur recommandÃ© : Gunicorn
- Configuration multi-workers
- Binding sur toutes les interfaces rÃ©seau

### Exemples AvancÃ©s

#### RequÃªte de GÃ©nÃ©ration
```bash
curl -X POST http://localhost:5001/generate \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Explique le machine learning"}'
```

#### RequÃªte RAG
```bash
curl -X POST http://localhost:5001/rag_query \
     -H "Content-Type: application/json" \
     -d '{"query": "Qu\'est-ce que le RAG ?"}'
```

#### RequÃªte d'Informations du ModÃ¨le
```bash
curl http://localhost:5001/model_info
```

### Perspectives d'AmÃ©lioration ğŸš€

1. Ajout de mÃ©triques de performance
2. Mise en place de la pagination pour les grandes rÃ©ponses
3. IntÃ©gration de mÃ©canismes de cache
4. DÃ©veloppement de tests unitaires et d'intÃ©gration
5. Mise en place de la surveillance des requÃªtes

## ğŸ’» PrÃ©requis Techniques

- Python 3.8+
- BibliothÃ¨ques :
  - `torch`
  - `transformers`
  - `chromadb`
  - `sentence-transformers`
  - `PyPDF2`

## ğŸš¦ Comment Utiliser

1. Installez les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

2. PrÃ©parez un fichier PDF

3. Lancez la dÃ©monstration :
   ```bash
   python llm_rag_demo.py
   ```

## ğŸ“ Pour les DÃ©butants

- **ModÃ¨le de Langage** : Un "assistant" qui gÃ©nÃ¨re du texte
- **Embedding** : Transformer du texte en "coordonnÃ©es" pour la recherche
- **SÃ©mantique** : Comprendre le sens, pas juste les mots

## ğŸ”¬ Points Techniques AvancÃ©s

- Utilisation de `torch` pour l'accÃ©lÃ©ration GPU
- Gestion dynamique du pÃ©riphÃ©rique (CPU/GPU)
- Logging colorÃ© pour une meilleure lisibilitÃ©

## ğŸš§ Limitations

- ModÃ¨le lÃ©ger (`distilgpt2`)
- Performances limitÃ©es pour des tÃ¢ches complexes
- NÃ©cessite un bon document source

## ğŸ¤ Contributions

N'hÃ©sitez pas Ã  ouvrir des issues ou proposer des amÃ©liorations !

## ğŸ“œ Licence

[SpÃ©cifiez votre licence]

## ğŸŒ API Flask avec Swagger

### FonctionnalitÃ©s de l'API

#### 1. GÃ©nÃ©ration de Texte `/generate`
- **MÃ©thode** : POST
- **Description** : GÃ©nÃ©rer du texte avec le modÃ¨le LLM
- **ParamÃ¨tres** :
  - `prompt` (requis) : Texte de dÃ©part pour la gÃ©nÃ©ration

#### 2. RequÃªte RAG `/rag_query`
- **MÃ©thode** : POST
- **Description** : Effectuer une recherche sÃ©mantique avec RAG
- **ParamÃ¨tres** :
  - `query` (requis) : Question ou requÃªte pour la recherche

#### 3. Informations du ModÃ¨le `/model_info`
- **MÃ©thode** : GET
- **Description** : Fournir des informations dÃ©taillÃ©es sur le modÃ¨le de langage utilisÃ©
- **ParamÃ¨tres** : Aucun

### DÃ©marrage de l'API

1. Installer les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

2. Lancer l'API :
   ```bash
   python app.py
   ```

3. AccÃ©der Ã  la documentation Swagger :
   - URL : `http://localhost:5001/apidocs/`

### Exemple de RequÃªte cURL

#### GÃ©nÃ©ration de Texte
```bash
curl -X POST http://localhost:5001/generate \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Explique le machine learning"}'
```

#### RequÃªte RAG
```bash
curl -X POST http://localhost:5001/rag_query \
     -H "Content-Type: application/json" \
     -d '{"query": "Qu\'est-ce que le RAG ?"}'
```

#### RequÃªte d'Informations du ModÃ¨le
```bash
curl http://localhost:5001/model_info
```

### DÃ©ploiement

- Serveur de production recommandÃ© : Gunicorn
- Commande de dÃ©marrage :
  ```bash
  gunicorn -w 4 -b 0.0.0.0:5001 app:app
  ```

### SÃ©curitÃ© et Configuration

- Mode debug dÃ©sactivÃ© en production
- Configuration CORS si nÃ©cessaire
- Ajout potentiel d'authentification

### Optimisation de la Base Vectorielle ğŸš€

#### StratÃ©gie d'Initialisation
- **Initialisation Unique** : La base vectorielle est crÃ©Ã©e une seule fois au dÃ©marrage de l'application
- **Persistance** : Stockage dans un rÃ©pertoire local `./vectorstore`
- **RÃ©utilisation** : La mÃªme collection est utilisÃ©e pour toutes les requÃªtes RAG

#### Avantages
1. **Performance AmÃ©liorÃ©e** : Ã‰vite la reconstruction Ã  chaque requÃªte
2. **Ã‰conomie de Ressources** : Minimise la charge CPU et mÃ©moire
3. **CohÃ©rence** : Maintient un contexte constant entre les requÃªtes

#### Gestion des Erreurs
- VÃ©rification de l'initialisation avant chaque requÃªte
- Logs dÃ©taillÃ©s en cas d'Ã©chec
- MÃ©canisme de reprise en cas de problÃ¨me

### Monitoring de la Base Vectorielle ğŸ“Š

#### Informations Disponibles
- Nombre total de segments indexÃ©s
- Chemin de stockage
- ModÃ¨le d'embedding utilisÃ©

#### Exemple de Log d'Initialisation
```
âœ… ModÃ¨le d'embedding initialisÃ©
âœ… Collection vectorielle initialisÃ©e avec 78 segments
```

### Perspectives d'AmÃ©lioration ğŸŒŸ
1. Mise en place d'un mÃ©canisme de rafraÃ®chissement pÃ©riodique
2. Ajout de mÃ©triques de performance
3. ImplÃ©mentation d'un systÃ¨me de cache intelligent

### CompatibilitÃ© des Embeddings ğŸ”—

#### ProblÃ©matique
- Changement rÃ©cent dans l'interface ChromaDB
- NÃ©cessitÃ© d'adapter la fonction d'embedding

#### Solution : Wrapper d'Embedding ğŸ› ï¸
```python
class EmbeddingFunctionWrapper:
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
    
    def __call__(self, input):
        # Conversion des embeddings pour ChromaDB
        embeddings = self.embedding_model.encode(input)
        return embeddings.tolist()
```

#### Avantages du Wrapper
1. **CompatibilitÃ©** : Interface standardisÃ©e avec ChromaDB
2. **FlexibilitÃ©** : Supporte diffÃ©rents modÃ¨les d'embedding
3. **Gestion des Erreurs** : Logging et remontÃ©e des exceptions

### DÃ©tails Techniques de l'Embedding ğŸ“

#### ModÃ¨le UtilisÃ©
- **Nom** : all-MiniLM-L6-v2
- **Type** : Sentence Transformer
- **DimensionnalitÃ©** : 384 dimensions

#### Processus d'Embedding
1. RÃ©ception d'une liste de textes
2. Encodage en vecteurs numÃ©riques
3. Conversion en format compatible ChromaDB

### Optimisations Futures ğŸš€
1. Support de modÃ¨les d'embedding dynamiques
2. Mise en cache des embeddings
3. MÃ©triques de performance de l'embedding

### Endpoint d'Informations du ModÃ¨le ğŸ”

#### `/model_info` - DÃ©tails Techniques du ModÃ¨le

##### Description
Fournit des informations dÃ©taillÃ©es sur le modÃ¨le de langage utilisÃ©.

##### Exemple de RÃ©ponse
```json
{
  "model_name": "gpt2",
  "model_size": {
    "total_parameters": 124_000_000,
    "trainable_parameters": 124_000_000,
    "estimated_size_mb": 480.47,
    "device": "mps"
  }
}
```

##### Informations RetournÃ©es
- **Nom du ModÃ¨le** : Type de modÃ¨le utilisÃ©
- **Nombre Total de ParamÃ¨tres**
- **ParamÃ¨tres EntraÃ®nables**
- **Taille EstimÃ©e** (en Mo)
- **PÃ©riphÃ©rique** (CPU, GPU, MPS)

#### Cas d'Usage
- Diagnostic de configuration
- Monitoring des ressources
- ComprÃ©hension des capacitÃ©s du modÃ¨le

### Exemple de RequÃªte
```bash
curl http://localhost:5001/model_info
```

### InterprÃ©tation des RÃ©sultats ğŸ“Š
- **ParamÃ¨tres** : Nombre de poids ajustables
- **Taille** : Estimation de l'empreinte mÃ©moire
- **PÃ©riphÃ©rique** : MatÃ©riel d'exÃ©cution
