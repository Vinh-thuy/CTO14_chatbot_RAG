import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import chromadb
from sentence_transformers import SentenceTransformer
import PyPDF2
import os
import logging
import colorlog
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Codes de couleurs ANSI
class LogColors:
    MAGENTA = '\033[35m'  # Chargement du mod√®le Hugging Face
    BLUE = '\033[34m'     # Requ√™te LLM
    GREEN = '\033[32m'    # Configuration du RAG
    CYAN = '\033[36m'     # Requ√™te RAG
    RESET = '\033[0m'     # R√©initialiser la couleur

def log_hf_llm_load(message):
    """Log les messages de chargement du mod√®le Hugging Face en magenta"""
    print(f"{LogColors.MAGENTA}ü§ñ {message}{LogColors.RESET}")

def log_llm_query(message):
    """Log les messages de requ√™te LLM en bleu"""
    print(f"{LogColors.BLUE}üîç {message}{LogColors.RESET}")

def log_rag_setup(message):
    """Log les messages de configuration du RAG en vert"""
    print(f"{LogColors.GREEN}üèóÔ∏è {message}{LogColors.RESET}")

def log_rag_query(message):
    """Log les messages de requ√™te RAG en cyan"""
    print(f"{LogColors.CYAN}üïµÔ∏è {message}{LogColors.RESET}")

# Configuration du logging avec couleurs et mise en forme am√©lior√©e
def setup_logger():
    """
    Configurer un logger personnalis√© avec des couleurs et un formatage am√©lior√©.
    
    Returns:
        logging.Logger: Logger configur√©
    """
    # Cr√©er un gestionnaire de console
    handler = colorlog.StreamHandler()
    
    # D√©finir le format du log avec des couleurs
    formatter = colorlog.ColoredFormatter(
        '%(log_color)s[%(levelname)s]%(reset)s '
        '%(blue)s%(message)s',
        log_colors={
            'DEBUG': 'cyan',
            'INFO': 'green',
            'WARNING': 'yellow',
            'ERROR': 'red',
            'CRITICAL': 'red,bg_white',
        }
    )
    handler.setFormatter(formatter)
    
    # Configurer le logger
    logger = colorlog.getLogger(__name__)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    
    return logger

# Initialiser le logger
logger = setup_logger()

def log_section_separator(title, symbol='=', length=80):
    """
    G√©n√®re un s√©parateur visuel pour les logs
    
    Args:
        title (str): Titre de la section
        symbol (str, optional): Symbole de s√©paration. Defaults to '='.
        length (int, optional): Longueur du s√©parateur. Defaults to 80.
    """
    separator = symbol * length
    centered_title = title.center(length)
    logger.info("\n" + separator)
    logger.info(centered_title)
    logger.info(separator + "\n")

# Mod√®le LLM s√©lectionn√©
model_name = "distilgpt2"

def load_llm():
    """
    Charge le mod√®le de langage avec des param√®tres optimis√©s et force le t√©l√©chargement
    
    Returns:
        tuple: (mod√®le, tokenizer)
    """
    import os
    import shutil
    import huggingface_hub

    # Chemin du cache personnalis√©
    custom_cache_dir = os.path.join(os.path.expanduser('~'), '.cache', 'custom_llm_cache')
    
    # Supprimer le cache existant pour forcer le t√©l√©chargement
    logger.info(f"üîÑ Pr√©paration du t√©l√©chargement forc√© du mod√®le : {model_name}")
    if os.path.exists(custom_cache_dir):
        logger.info(f"üóëÔ∏è Suppression du cache existant : {custom_cache_dir}")
        shutil.rmtree(custom_cache_dir)
    
    os.makedirs(custom_cache_dir, exist_ok=True)
    
    # √âtape 1 : T√©l√©chargement et chargement du tokenizer
    logger.info("ü§ñ T√©l√©chargement du tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            cache_dir=custom_cache_dir,
            use_fast=True,
            force_download=True,
            resume_download=False
        )
        
        # Configuration du tokenizer
        tokenizer.pad_token = tokenizer.eos_token
        logger.info(f"‚úÖ Tokenizer t√©l√©charg√© : {tokenizer.__class__.__name__}")
        logger.info(f"üìä Taille du vocabulaire : {len(tokenizer.vocab)} tokens")
        
        # D√©tails des tokens sp√©ciaux
        special_tokens = {
            'bos_token': tokenizer.bos_token, 
            'eos_token': tokenizer.eos_token, 
            'unk_token': tokenizer.unk_token,
            'pad_token': tokenizer.pad_token
        }
        logger.info("üè∑Ô∏è Tokens sp√©ciaux : " + str(special_tokens))
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du t√©l√©chargement du tokenizer : {e}")
        raise
    
    # √âtape 2 : T√©l√©chargement et chargement du mod√®le
    logger.info("ü§ñ T√©l√©chargement du mod√®le de langage...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=custom_cache_dir,
            device_map='auto',
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            force_download=True,
            resume_download=False,
            attn_implementation='eager'
        )
        
        # Configuration du mod√®le
        model.config.pad_token_id = tokenizer.pad_token_id
        
        # Informations d√©taill√©es sur le mod√®le
        logger.info(f"‚úÖ Mod√®le t√©l√©charg√© : {model.__class__.__name__}")
        logger.info(f"üìä Nombre de param√®tres : {model.num_parameters():,}")
        
        # V√©rification de la taille du mod√®le
        import glob
        model_files = glob.glob(os.path.join(custom_cache_dir, '*.safetensors')) + \
                      glob.glob(os.path.join(custom_cache_dir, 'pytorch_model.bin'))
        
        if model_files:
            model_size_mb = os.path.getsize(model_files[0]) / (1024 * 1024)
            logger.info(f"üíæ Taille du mod√®le : {model_size_mb:.2f} Mo")
        else:
            logger.warning("‚ö†Ô∏è Impossible de d√©terminer la taille du mod√®le")
        
        # Informations sur le p√©riph√©rique
        logger.info(f"üíª P√©riph√©rique : {'MPS' if torch.backends.mps.is_available() else 'CPU'}")
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du t√©l√©chargement du mod√®le : {e}")
        raise
    
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_length=500):
    """
    G√©n√®re une r√©ponse textuelle avec des param√®tres adapt√©s au mod√®le
    
    Args:
        model: Mod√®le de g√©n√©ration de texte
        tokenizer: Tokenizer associ√©
        prompt: Texte d'entr√©e
        max_length: Longueur maximale de la r√©ponse
    
    Returns:
        str: R√©ponse g√©n√©r√©e
    """
    log_llm_query("D√©tails de la requ√™te LLM")
    log_llm_query(f"Prompt original : {prompt}")
    
    try:
        # Configurer le p√©riph√©rique
        if torch.backends.mps.is_available():
            device = torch.device("cpu")  # Forcer CPU pour √©viter les probl√®mes MPS
        else:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # D√©placer le mod√®le sur le bon p√©riph√©rique
        model = model.to(device)
        
        # Pr√©paration de l'entr√©e
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096).to(device)
        log_llm_query(f"Analyse du prompt :")
        log_llm_query(f"Nombre de tokens : {len(inputs['input_ids'][0])}")
        
        # Configuration de la g√©n√©ration
        log_llm_query("Param√®tres de g√©n√©ration :")
        log_llm_query("Mode : G√©n√©ration avec √©chantillonnage")
        log_llm_query(f"Longueur max : {max_length} tokens")
        
        # G√©n√©ration de la r√©ponse
        outputs = model.generate(
            **inputs, 
            max_length=max_length, 
            num_return_sequences=1, 
            do_sample=True, 
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            no_repeat_ngram_size=2,
            pad_token_id=tokenizer.eos_token_id
        )
        
        # D√©codage de la r√©ponse
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        log_llm_query("R√©sultat de la g√©n√©ration :")
        log_llm_query(f"Longueur : {len(response)} caract√®res")
        log_llm_query(f"D√©but de la r√©ponse : {response[:100]}...")
        
        return response
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration : {e}")
        return f"Erreur de g√©n√©ration : {str(e)}"

def extract_pdf_text(pdf_path):
    """
    Extrait le texte d'un fichier PDF.
    
    √âtapes :
    1. Ouvrir le fichier PDF
    2. Lire chaque page
    3. Concat√©ner le texte
    
    Args:
        pdf_path: Chemin vers le fichier PDF
    
    Returns:
        str: Texte extrait du PDF
    """
    logger.info(f"Extraction du texte du PDF : {pdf_path}")
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
        
        logger.info(f"Extraction r√©ussie : {len(text)} caract√®res extraits")
        return text
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction PDF : {e}")
        return ""

def setup_rag_system(pdf_path):
    """
    Configuration du syst√®me RAG avec indexation de document PDF
    
    Args:
        pdf_path (str): Chemin vers le fichier PDF √† indexer
    
    Returns:
        chromadb.Collection: Collection vectorielle pour la recherche s√©mantique
    """
    logger.info("üèóÔ∏è Configuration d√©taill√©e du syst√®me RAG")
    logger.info(f"üèóÔ∏è Source de connaissances : {pdf_path}")
    
    # Initialisation de ChromaDB
    logger.info("üèóÔ∏è Initialisation de la base de donn√©es vectorielle")
    client = chromadb.PersistentClient(path="./chroma_storage")
    
    # Cr√©er ou r√©cup√©rer une collection
    collection_name = "lightrag_collection"
    try:
        collection = client.get_or_create_collection(
            name=collection_name, 
            metadata={"hnsw:space": "cosine"}
        )
        logger.info("üèóÔ∏è Collection ChromaDB :")
        logger.info("üèóÔ∏è Cr√©ation : R√©ussi")
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation de la collection : {e}")
        raise
    
    # Extraction du texte du PDF
    text = extract_pdf_text(pdf_path)
    logger.info(f"[INFO] Extraction du texte du PDF : {pdf_path}")
    logger.info(f"[INFO] Extraction r√©ussie : {len(text)} caract√®res extraits")
    
    # Configuration du mod√®le d'embedding
    logger.info("üèóÔ∏è Analyse du texte source :")
    logger.info(f"üèóÔ∏è Nombre total de caract√®res : {len(text):,}")
    
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    logger.info("üèóÔ∏è Mod√®le d'embedding :")
    logger.info("üèóÔ∏è Nom : all-MiniLM-L6-v2")
    
    # Segmentation du texte
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200, 
        length_function=len
    )
    text_chunks = text_splitter.split_text(text)
    
    logger.info("üèóÔ∏è Segmentation du texte :")
    logger.info(f"üèóÔ∏è Nombre de segments : {len(text_chunks)}")
    
    # Embedding et indexation des segments
    for i, chunk in enumerate(text_chunks):
        embedding = embedder.encode(chunk).tolist()
        collection.add(
            ids=[f"chunk_{i}"],
            embeddings=[embedding],
            documents=[chunk]
        )
    
    logger.info("üèóÔ∏è R√©sum√© de l'indexation RAG :")
    logger.info(f"üèóÔ∏è Segments index√©s : {len(text_chunks)}")
    logger.info(f"üèóÔ∏è Taille de la collection : {len(text_chunks)} segments")
    
    return collection, embedder

def rag_query(collection, embedder, model, tokenizer, query, max_context_length=1000):
    """
    Effectue une requ√™te RAG avec recherche s√©mantique et g√©n√©ration de r√©ponse
    
    Args:
        collection (chromadb.Collection): Collection vectorielle
        embedder (SentenceTransformer): Mod√®le d'embedding
        model (transformers.PreTrainedModel): Mod√®le de g√©n√©ration
        tokenizer (transformers.PreTrainedTokenizer): Tokenizer associ√©
        query (str): Question ou requ√™te de l'utilisateur
        max_context_length (int, optional): Longueur maximale du contexte. Defaults to 1000.
    
    Returns:
        str: R√©ponse g√©n√©r√©e avec contexte
    """
    logger.info("üïµÔ∏è Requ√™te RAG d√©taill√©e")
    logger.info(f"üïµÔ∏è Question : {query}")
    
    # Embedding de la requ√™te
    query_embedding = embedder.encode(query).tolist()
    logger.info("üïµÔ∏è Embedding de la requ√™te :")
    logger.info(f"üïµÔ∏è Dimension : {len(query_embedding)}")
    
    # Recherche de contexte
    logger.info("üïµÔ∏è Recherche de contexte :")
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3  # R√©cup√©rer les 3 segments les plus pertinents
    )
    
    # Extraction des segments pertinents
    context_segments = results['documents'][0]
    logger.info(f"üïµÔ∏è Segments trouv√©s : {len(context_segments)}")
    
    # Analyse du contexte
    context = " ".join(context_segments)
    logger.info("üïµÔ∏è Analyse du contexte :")
    logger.info(f"üïµÔ∏è Longueur : {len(context)} caract√®res")
    logger.info(f"üïµÔ∏è D√©but du contexte : {context[:100]}...")
    
    # Construction du prompt enrichi
    enriched_prompt = f"""Contexte: {context}

Question: {query}

R√©ponds de mani√®re pr√©cise et d√©taill√©e en te basant uniquement sur le contexte fourni :"""
    
    logger.info("üïµÔ∏è G√©n√©ration de r√©ponse RAG :")
    logger.info("üïµÔ∏è Construction du prompt enrichi")
    logger.info(f"üïµÔ∏è Longueur du prompt : {len(enriched_prompt)} caract√®res")
    
    # G√©n√©rer une r√©ponse avec le contexte
    return generate_response(model, tokenizer, enriched_prompt, max_length=300)

def compare_responses(response_without_context, rag_response):
    """
    Compare les r√©ponses g√©n√©r√©es sans et avec contexte RAG
    
    Args:
        response_without_context (str): R√©ponse g√©n√©r√©e sans contexte
        rag_response (str): R√©ponse g√©n√©r√©e avec contexte RAG
    """
    logger.info("\nüîç Analyse comparative des r√©ponses")
    
    # Calcul des m√©triques de base
    without_context_length = len(response_without_context)
    rag_response_length = len(rag_response)
    
    # Analyse de la pertinence
    logger.info(f"üìè Longueur de la r√©ponse sans contexte : {without_context_length} caract√®res")
    logger.info(f"üìè Longueur de la r√©ponse avec contexte RAG : {rag_response_length} caract√®res")
    
    # Comparaison qualitative
    if rag_response_length > without_context_length:
        logger.info("‚úÖ La r√©ponse RAG semble plus d√©taill√©e")
    elif rag_response_length < without_context_length:
        logger.info("ü§î La r√©ponse RAG est plus concise que la r√©ponse initiale")
    else:
        logger.info("üîÑ Les r√©ponses ont une longueur similaire")
    
    # Indication de la valeur ajout√©e du RAG
    logger.info("\nüí° Valeur ajout√©e du RAG :")
    logger.info("1. Contextualisation : Enrichissement de la r√©ponse avec des informations sp√©cifiques")
    logger.info("2. Pr√©cision : R√©duction des hallucinations et des informations g√©n√©riques")
    logger.info("3. Adaptabilit√© : Capacit√© √† fournir des r√©ponses bas√©es sur des sources sp√©cifiques")

def manual_step_validation(step_name, description=None):
    """
    Permet √† l'utilisateur de valider manuellement le passage √† l'√©tape suivante
    
    Args:
        step_name (str): Nom de l'√©tape en cours
        description (str, optional): Description suppl√©mentaire de l'√©tape
    
    Returns:
        bool: True si l'utilisateur valide, False sinon
    """
    try:
        logger.info(f"\nüîç Validation manuelle de l'√©tape : {step_name}")
        if description:
            logger.info(f"üìù Description : {description}")
        
        while True:
            user_input = input("‚ùì Voulez-vous continuer ? (o/n) : ").strip().lower()
            
            if user_input in ['o', 'oui', 'y', 'yes']:
                logger.info("‚úÖ √âtape valid√©e par l'utilisateur")
                return True
            elif user_input in ['n', 'non', 'no']:
                logger.info("üõë √âtape annul√©e par l'utilisateur")
                return False
            else:
                logger.warning("‚ùå R√©ponse invalide. Veuillez r√©pondre par 'o' ou 'n'.")
    
    except KeyboardInterrupt:
        logger.info("\nüö´ Interruption manuelle d√©tect√©e")
        return False

def main():
    """
    Fonction principale orchestrant la d√©monstration LLM et RAG
    """
    # Initialisation et configuration du syst√®me
    logger.info("üöÄ Initialisation du syst√®me de RAG avanc√©")
    logger.info("üîç Pr√©paration des composants essentiels : mod√®le LLM, RAG, et outils d'analyse")
    
    log_section_separator("√âtape 1 : Chargement du mod√®le LLM")
    # √âtape 1 : Chargement du mod√®le LLM
    logger.info("\nü§ñ √âTAPE 1 : CHARGEMENT DU MOD√àLE DE LANGAGE")
    logger.info("üí° Logique : Initialiser un mod√®le de langage capable de comprendre et g√©n√©rer du texte.")
    logger.info("   - S√©lection d'un mod√®le compact et performant")
    logger.info("   - Configuration optimis√©e pour les ressources limit√©es")
    model, tokenizer = load_llm()
    
    if not manual_step_validation(
        "√âtape 1 : Chargement du mod√®le LLM", 
        description="V√©rifiez les d√©tails du mod√®le charg√© : type, nombre de param√®tres, tokens sp√©ciaux"
    ):
        return
    
    log_section_separator("√âtape 2 : R√©ponse LLM sans contexte")
    # √âtape 2 : R√©ponse LLM sans contexte
    logger.info("\nüß† √âTAPE 2 : G√âN√âRATION DE R√âPONSE SANS CONTEXTE")
    logger.info("üí° Logique : Tester les capacit√©s brutes du mod√®le sans information suppl√©mentaire.")
    logger.info("   - √âvaluer la compr√©hension g√©n√©rale du mod√®le")
    logger.info("   - Observer les limites et potentialit√©s du mod√®le")
    prompt = "Explique moi la solution que LightRAG apporte et ses forces ?"
    response_without_context = generate_response(model, tokenizer, prompt)
    logger.info(f"R√©ponse sans contexte : {response_without_context}")
    
    if not manual_step_validation(
        "√âtape 2 : R√©ponse LLM sans contexte", 
        description="Examinez la r√©ponse g√©n√©r√©e sans contexte sp√©cifique. Observez la qualit√© et la coh√©rence."
    ):
        return
    
    log_section_separator("√âtape 3 : Configuration du syst√®me RAG")
    # √âtape 3 : Configuration du syst√®me RAG
    logger.info("\nüåê √âTAPE 3 : CONFIGURATION DU SYST√àME RAG")
    logger.info("üí° Logique : Pr√©parer un syst√®me de Retrieval-Augmented Generation (RAG).")
    logger.info("   - Charger et indexer une source de connaissances")
    logger.info("   - Cr√©er une base de donn√©es vectorielle pour la recherche s√©mantique")
    pdf_path = "LightRAG.pdf"
    rag_collection, embedder = setup_rag_system(pdf_path)
    
    if not manual_step_validation(
        "√âtape 3 : Configuration du syst√®me RAG", 
        description="V√©rifiez la configuration du syst√®me RAG : nombre de segments, mod√®le d'embedding"
    ):
        return
    
    log_section_separator("√âtape 4 : R√©ponse LLM avec contexte RAG")
    # √âtape 4 : R√©ponse LLM avec contexte RAG
    logger.info("\nüî¨ √âTAPE 4 : G√âN√âRATION DE R√âPONSE AVEC CONTEXTE RAG")
    logger.info("üí° Logique : Enrichir la g√©n√©ration de r√©ponse avec des connaissances contextuelles.")
    logger.info("   - Rechercher des segments pertinents dans la base de connaissances")
    logger.info("   - Augmenter le prompt avec des informations contextuelles")
    rag_response = rag_query(rag_collection, embedder, model, tokenizer, prompt)
    logger.info(f"R√©ponse avec contexte RAG : {rag_response}")
    
    if not manual_step_validation(
        "√âtape 4 : R√©ponse LLM avec contexte RAG", 
        description="Comparez la r√©ponse avec contexte RAG √† la r√©ponse pr√©c√©dente. Notez les diff√©rences."
    ):
        return
    
    log_section_separator("√âtape 5 : Analyse comparative")
    # √âtape 5 : Analyse comparative
    logger.info("\nüìä √âTAPE 5 : ANALYSE COMPARATIVE DES R√âPONSES")
    logger.info("üí° Logique : Comparer et √©valuer les r√©ponses g√©n√©r√©es.")
    logger.info("   - Mesurer l'impact du contexte sur la qualit√© de la r√©ponse")
    logger.info("   - Identifier les am√©liorations apport√©es par le RAG")
    compare_responses(response_without_context, rag_response)
    
    log_section_separator("Conclusion : D√©monstration RAG", symbol='*')
    logger.info("üåü R√©sum√© de la d√©monstration :")
    logger.info("   1. Mod√®le LLM : DistilGPT2")
    logger.info("   2. Syst√®me RAG : Indexation et recherche s√©mantique")
    logger.info("   3. Am√©lioration de la r√©ponse : Contextualisation")
    
    log_section_separator("Merci !", symbol='~', length=40)
    logger.info("\nüèÅ D√âMONSTRATION RAG TERMIN√âE")
    logger.info("Merci d'avoir explor√© les capacit√©s de notre syst√®me RAG !")

if __name__ == "__main__":
    main()
